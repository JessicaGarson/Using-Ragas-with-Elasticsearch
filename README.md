# ragas
RAGAS is a framework for evaluating RAG pipelines with human-aligned metrics. This repostiory contains a demo using a sample book dataset.

For more details, see the [Ragas GitHub](https://github.com/explodinggradients/ragas).

## Dataset

## Contents

- **ragas-testing.ipynb**: Main Jupyter notebook for running RAGAS evaluations. It sets up the environment, loads data, runs sample queries, and computes evaluation metrics (context precision, faithfulness, answer relevancy) using Ragas.
- **ragas_evaluation_results.csv**: Output file generated by the notebook, containing detailed results for each evaluation query, including metrics for context precision, faithfulness, and answer relevancy.
- **LICENSE**: License for the code and resources in this folder.
- **.gitignore**: Git ignore rules for this folder.

## Usage

1. **Install Dependencies**
   The notebook will install required dependencies automatically, but you can also install them manually:
   
   ```bash
   pip install ragas datasets langchain elasticsearch openai langchain-openai
   ```
   You will also need an OpenAI API key for LLM-based metrics. Set it as an environment variable:

   ```bash
   export OPENAI_API_KEY=your-key-here
   ```

2. **Run the Notebook**
   Open `ragas-testing.ipynb` in Jupyter and follow the instructions to run each cell. The notebook will:
   - Query your book index (via Elasticsearch)
   - Run sample RAG queries
   - Evaluate the responses using RAGAS
   - Output results to `ragas_evaluation_results.csv`

3. **View Results**
   The results file contains detailed metrics for each test query. Use it to analyze the quality of your RAG pipeline and compare different configurations.
